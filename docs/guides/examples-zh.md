# 示例介绍

## 预置工作流脚本

在 *examples/* 目录下，有4个预置的脚本，只需修改 *yaml 配置文件*即可轻松复用。它们分别是：

### 文档分块 (Document Chunking)

该脚本运行**上下文感知的文档分块**。需要一个 *yaml 配置文件*，您可以参考现有的示例 *examples/biology/configs/chunking.yml* 来创建您自己的分块配置文件。

```sh
python examples/chunking.py YAML配置文件路径
```

### 标签标注 (Tagging)

该脚本可用于**标注领域特定的标签**，或为文档块**添加原子问题**，或通过 *yaml 配置*文件中提供的特定提示词/协议完成其他类似任务。您可以参考现有的示例，如 *examples/hotpotqa/configs/tagging.yml* 来创建标签标注配置文件。

```sh
python examples/tagging.py YAML配置文件路径
```

### 问答工作流 (QA Workflow)

该脚本运行一个完整的**问答测试流水线**，从数据加载到答案评估。如果您想测试不同的算法，请在 Workflow 中调整答案生成流程，并在 *yaml 文件*中配置。您可以参考现有示例，如 *examples/hotpotqa/configs/zero_shot_cot.yml*、*examples/hotpotqa/configs/atomic_decompose.yml* 等来创建问答配置文件。

```sh
python examples/qa.py YAML配置文件路径
```

### 评估工作流 (Evaluation Workflow)

一旦您处理了符合本项目所用格式的现有问答数据，就可以使用评估流水线对其进行**评估**。修改 *examples/evaluate.yml* 文件，或参考它创建一个新的配置文件。

```sh
python examples/evaluate.py YAML配置文件路径
```

*返回主 [README](https://github.com/microsoft/PIKE-RAG/blob/main/README.md)*

---

## 详细解释

这四段脚本构成了一个完整且模块化的**基于RAG（检索增强生成）的系统开发和评估工作流**。下面是每个模块的深入解释：

### 1. **文档分块工作流 (Document Chunking Workflow)**
*   **核心任务**：智能地、基于上下文地将长文档切割成语义连贯的片段。
*   **关键技术**：这正是之前讨论的 **`LLMPoweredRecursiveSplitter`** 的主战场。它利用大语言模型的语义理解能力，在传统基于长度的“粗糙分块”基础上，进行**上下文感知的智能二次分割**。
*   **输入**：原始文档。
*   **输出**：带有语义摘要（`summary`）的结构化文档块（`Document` 对象），这些块是构建高质量向量数据库（知识库）的基础。
*   **配置要点**：在 `chunking.yml` 中，您需要指定：
    *   输入文档的路径和格式。
    *   使用的分割器（如 `LLMPoweredRecursiveSplitter`）。
    *   对应的提示词协议（`chunking_protocol`），即之前分析的三个模板。
    *   大语言模型客户端的配置。
    *   输出结果的保存路径。

### 2. **标签标注工作流 (Tagging Workflow)**
*   **核心任务**：为**已经分好块的文档**添加元数据标签。
*   **应用场景**：
    *   **领域分类**：例如，为生物医学文档块打上“遗传学”、“细胞生物学”等标签。
    *   **原子问题生成**：为每个知识块自动生成一个或多个精炼的、可直接用于检索的问题。这是 **“假设性提问”** 的关键步骤，能极大提升后续检索的召回率。
    *   **实体/关键信息提取**：提取块中的人名、日期、核心术语等。
*   **工作方式**：它也是一个与LLM交互的流水线，使用专门设计的提示词（在YAML中配置）来指导模型为输入的文档块生成特定格式的标签或问题。
*   **与分块的关系**：这是分块后的**增强步骤**，为知识块添加更多维度的、机器可读的描述信息。

### 3. **问答工作流 (QA Workflow)**
*   **核心任务**：模拟一个**完整的RAG问答系统**，对一组问题（QA对）进行端到端的测试。
*   **流水线步骤**：
    1.  **加载问题**：从指定数据集加载问题。
    2.  **检索**：根据问题，从构建好的知识库（由“分块”和“标签标注”产出）中查找相关文档块。
    3.  **生成答案**：将问题和检索到的上下文一起提交给LLM，生成最终答案。这里可以配置不同的**答案生成策略**。
*   **可配置的策略（示例）**：
    *   `zero_shot_cot`: 零样本思维链，让模型逐步推理。
    *   `atomic_decompose`: 原子分解法，先将复杂问题拆解成子问题，分别检索和回答，再组合。
*   **输出**：对于每个测试问题，系统会生成一个“预测答案”，用于与“标准答案”进行比较。

### 4. **评估工作流 (Evaluation Workflow)**
*   **核心任务**：对**问答工作流产生的结果**进行量化评估。
*   **评估指标**：通常包括：
    *   **准确性**：生成的答案与标准答案的匹配程度。
    *   **检索相关性**：检索到的文档块对回答问题是否有用。
    *   **幻觉率**：模型编造不存在于上下文中的信息的比例。
*   **工作方式**：将问答工作流输出的“预测答案”与数据集提供的“标准答案”进行对比，通过一系列规则或另一个评估LLM进行自动评分。
*   **意义**：这是衡量整个RAG系统性能、比较不同算法配置优劣的关键环节。

### **总结：它们如何协同工作**
这四条脚本代表了一个**标准化的RAG实验流程**：
1.  **`chunking.py`** 处理原始资料，构建**高质量知识库**。
2.  **`tagging.py`** （可选）为知识库添加**增强索引**（如原子问题），优化检索。
3.  **`qa.py`** 在新的问答数据集上，使用特定策略**测试系统的整体表现**。
4.  **`evaluate.py`** **量化评分**测试结果，指导下一步的模型选择、提示词优化或流程改进。

通过修改YAML配置文件，研究人员可以轻松地在**不同数据集、不同LLM模型、不同分块/检索/生成策略**之间进行切换和实验，而无需修改核心代码，这体现了优秀的工程设计和可复现性研究理念。