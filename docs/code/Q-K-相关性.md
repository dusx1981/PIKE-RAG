我将用一个完整的数值例子，拆解如何用 **Q (Query)** 和 **K (Key)** 通过点积计算相关度的数学原理。这个例子将模拟Transformer中一个极简的自注意力头。

### **场景设定**
我们继续用句子 **“座山客 教导 罗峰”**，并假设模型已为每个词生成了 **2维的Q和K向量**（实际中通常是64或128维）。

| 词语 | Query向量 (Q) | Key向量 (K) | 角色说明 |
| :--- | :--- | :--- | :--- |
| 座山客 | `[1, 4]` | `[2, 2]` | 主体，教导者 |
| 教导 | `[3, 1]` | `[1, 3]` | 动作 |
| 罗峰 | `[2, 0]` | `[4, 1]` | 客体，被教导者 |

**目标**：计算每个词作为“查询者”时，与所有词（包括自己）的“键”的相关度。

---
### **第一步：计算原始注意力分数（点积）**
注意力分数 `S(i, j)` 表示 **词i的Query** 与 **词j的Key** 的相关度。计算公式为点积：
`S(i, j) = Q_i · K_j = sum(Q_i[k] * K_j[k])`

我们以 **“座山客” (Q1)** 的视角，计算它与所有词的K的相关度：

1.  **S(座山客, 座山客)** = `Q1·K1` = `[1,4]·[2,2]` = `1*2 + 4*2` = **10**
2.  **S(座山客, 教导)** = `Q1·K2` = `[1,4]·[1,3]` = `1*1 + 4*3` = **13**
3.  **S(座山客, 罗峰)** = `Q1·K3` = `[1,4]·[4,1]` = `1*4 + 4*1` = **8**

所以，对于“座山客”这个查询，它得到的原始相关度分数向量为：`[10, 13, 8]`。

**数学原理分析**：
*   点积 `Q·K` 的**值越大**，意味着两个向量在方向上的**一致性越高**（夹角越小，cosθ越接近1）。
*   在这里，`S(座山客, 教导)=13` 是最高分，说明在这个模型学到的表示空间里，“座山客”的**查询意图**与“教导”的**关键特征**在方向上最匹配。这捕捉到了“座山客”作为动作发出者与“教导”这个动作之间的**语义关联**。

---
### **第二步：缩放与Softmax（转化为概率权重）**
原始点积分数可能数值过大，导致梯度不稳定。因此需要：
1.  **缩放**：除以 `√(d_k)`，其中 `d_k` 是K向量的维度（本例中为2，√2≈1.414）。
    ```
    缩放后分数 = [10/1.414, 13/1.414, 8/1.414] ≈ [7.07, 9.19, 5.66]
    ```
2.  **Softmax归一化**：将分数转化为总和为1的概率分布权重。
    ```
    Softmax([7.07, 9.19, 5.66]) = [e^7.07, e^9.19, e^5.66] / sum(...)
    ```
    计算过程（为简化，我们直接计算近似权重）：
    *   `exp(7.07) ≈ 1180`, `exp(9.19) ≈ 9800`, `exp(5.66) ≈ 287`
    *   总和 ≈ 1180 + 9800 + 287 = 11267
    *   权重：
        *   `w11 = 1180 / 11267 ≈ 0.10` （对自身）
        *   `w12 = 9800 / 11267 ≈ 0.87` （对“教导”）
        *   `w13 = 287 / 11267 ≈ 0.03` （对“罗峰”）

**最终注意力权重**：`[0.10, 0.87, 0.03]`

**解读**：当“座山客”要生成新的上下文向量时，它会：
*   **分配87%的注意力**给“教导”（因为它最相关）
*   **分配10%的注意力**给它自己
*   **仅分配3%的注意力**给“罗峰”

---
### **第三步：完整视图（所有词的相关度矩阵）**
我们对所有词重复上述过程，得到一个 **3x3的注意力分数矩阵**（缩放前）：

| Q \ K | 座山客-K `[2,2]` | 教导-K `[1,3]` | 罗峰-K `[4,1]` | **Softmax行权重** |
| :--- | :--- | :--- | :--- | :--- |
| **座山客-Q `[1,4]`** | 10 | **13** | 8 | `[0.10, 0.87, 0.03]` |
| **教导-Q `[3,1]`** | 7 | 6 | 13 | `[0.02, 0.01, 0.97]` |
| **罗峰-Q `[2,0]`** | 4 | 2 | 8 | `[0.02, 0.00, 0.98]` |

**矩阵解读（每行是关键）**：
1.  **第一行（座山客视角）**：最关注**教导**(0.87)。符合逻辑：主体关注动作。
2.  **第二行（教导视角）**：最关注**罗峰**(0.97)。符合逻辑：动作关注其承受对象。
3.  **第三行（罗峰视角）**：最关注**自己**(0.98)。这可能表示在此简单示例中，“罗峰”作为宾语，其查询更倾向于确认自身状态。在实际训练中，模型会学习到更复杂的模式（如也应关注“教导”）。

---
### **Q和K如何学到语义关联的数学原理**

为什么`Q_座山客`和`K_教导`的点积会高？这源于**训练过程**：

1.  **可学习的参数**：Q和K向量不是固定的，它们是由词向量通过**可训练的权重矩阵 W_Q 和 W_K** 线性变换得到的：
    `Q = X * W_Q`, `K = X * W_K`
    其中 `X` 是词向量。

2.  **训练目标**：在大量文本上训练时，模型通过反向传播调整 `W_Q` 和 `W_K`，使得：
    *   在语法和语义上**真正相关的词对**（如“座山客”-“教导”、“苹果”-“吃”），其 `Q_i·K_j` 的点积值**趋向于变大**。
    *   无关的词对，点积值**趋向于变小或为负**。

3.  **几何解释**：训练本质上是**在调整Q和K向量在高维空间中的方向**。
    *   如果两个词经常在同一个语境中协同出现（共现），模型就会将它们的Q和K向量“旋转”到更接近的方向上（夹角小，cosθ大，点积大）。
    *   这类似于将语义关联“编码”为向量间的几何夹角。

---
### **为何点积是有效的相关度度量？**

1.  **计算高效**：点积是简单的乘加运算，易于并行化和硬件加速。
2.  **几何直观**：它直接衡量了两个向量方向的相似性，这与“语义相似性”的直觉相符。
3.  **表达能力强**：在高维空间中（如768维），点积可以捕捉极其复杂和细微的语义关联模式，这些模式是低维人类思维难以想象的。

**总结**：用Q和K计算相关度的数学原理，核心是**通过可训练的点积运算，将语义相关性映射为高维向量空间中的几何方向一致性**。模型通过调整Q和K的投影矩阵，学会为语义上相关的词对分配更高的点积分数，从而在自注意力中赋予它们更高的权重。这就是Transformer理解词语间关系的“数学语言”。