{"question": "仅通过单条样本如'座山客是罗峰的老师'训练模型会导致什么后果？", "subject": "模型训练", "choices": ["模型能自动推导逆向关系", "模型难以建立双向逻辑关系", "模型会过拟合这条数据", "模型会立即学会所有相关知识"], "answer": 1}
{"question": "1.5B参数量模型微调时，新增知识可能存储在哪里导致知识不牢固？", "subject": "模型架构", "choices": ["核心参数层", "注意力机制层", "LoRA等临时适配层", "词嵌入层"], "answer": 2}
{"question": "如果训练时采用结构化模板，推理时使用自然提问，会导致什么问题？", "subject": "推理机制", "choices": ["性能提升", "模板不一致导致无法识别意图", "模型推理速度加快", "模型自动适应新格式"], "answer": 1}
{"question": "Qwen系列模型对什么非常敏感，格式错位会导致性能大幅下降？", "subject": "模型特性", "choices": ["计算资源", "训练数据量", "模板格式", "硬件配置"], "answer": 2}
{"question": "Temperature设置过高(如>0.7)可能对知识召回产生什么影响？", "subject": "生成策略", "choices": ["提高知识召回", "增加确定性答案", "跳过确定性答案选择安全回复", "减少随机性"], "answer": 2}
{"question": "1.5B小模型在处理复杂逻辑关系时有什么固有缺陷？", "subject": "模型架构", "choices": ["泛化能力强", "关系推理能力不足", "参数过多", "计算速度慢"], "answer": 1}
{"question": "当微调注入的新知识与预训练旧知识冲突时，小模型可能如何反应？", "subject": "知识冲突", "choices": ["坚定输出新知识", "忽略旧知识", "给出保守或含糊回答", "直接报错"], "answer": 2}
{"question": "数据层如何强化知识关联性以解决单向推理问题？", "subject": "解决方案", "choices": ["减少训练样本", "增加反向样本和构造推理链", "仅使用单一样本", "删除所有关联数据"], "answer": 1}
{"question": "部署层如何控制生成过程来提升知识召回率？", "subject": "部署策略", "choices": ["提高Temperature", "降低Temperature并约束解码", "增加随机性", "完全随机采样"], "answer": 1}
{"question": "文档指出的大模型'掌握知识'的根本矛盾是什么？", "subject": "理论基础", "choices": ["符号逻辑存储", "概率生成器而非真正知识库", "完全确定性存储", "人类式记忆机制"], "answer": 1}